{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from joblib import dump\n",
    "\n",
    "\n",
    "X_all = pd.read_pickle('features.pickle')\n",
    "y_all = pd.read_pickle('labels.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.33, random_state=42\n",
    ")\n",
    "\n",
    "# Show the results of the split\n",
    "print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(X_test.shape[0]))\n",
    "\n",
    "print(\"Anomaly rate of the training set: {:.2f}%\"\n",
    "      .format(100 * (y_train == 1).mean()))\n",
    "print(\"Anomaly rate of the testing set: {:.2f}%\"\n",
    "      .format(100 * (y_test == 1).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating Models\n",
    "- Setup helper functions\n",
    "\n",
    "### Setup\n",
    "Run the code cell below to initialize three helper functions which you can use for training and testing supervised learning models. The functions are as follows:\n",
    "- `train_classifier` - takes as input a classifier and training data and fits the classifier to the data.\n",
    "- `predict_labels` - takes as input a fit classifier, features, and a target labeling and makes predictions using the F<sub>1</sub> score.\n",
    "- `train_predict` - takes as input a classifier, and the training and testing data, and performs `train_clasifier` and `predict_labels`.\n",
    " - This function will report the F<sub>1</sub> score for both the training and testing data separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(clf, X_train, y_train):\n",
    "    ''' Fits a classifier to the training data. '''\n",
    "\n",
    "    # Start the clock, train the classifier, then stop the clock\n",
    "    start = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time()\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Trained model in {:.4f} seconds\".format(end - start))\n",
    "\n",
    "\n",
    "def predict_labels(clf, features, target):\n",
    "    ''' Makes predictions using a fit classifier based on F1 score. '''\n",
    "\n",
    "    # Start the clock, make predictions, then stop the clock\n",
    "    start = time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time()\n",
    "\n",
    "    # Print and return results\n",
    "    print(\"Made predictions in {:.4f} seconds.\".format(end - start))\n",
    "    return f1_score(target.values, y_pred)\n",
    "\n",
    "\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    ''' Train and predict using a classifer based on F1 score. '''\n",
    "\n",
    "    # Indicate the classifier and the training set size\n",
    "    print(\"Training a {} using a training set size of {}. . .\"\n",
    "          .format(clf.__class__.__name__, len(X_train)))\n",
    "\n",
    "    # Train the classifier\n",
    "    train_classifier(clf, X_train, y_train)\n",
    "\n",
    "    # Print the results of prediction for both training and testing\n",
    "    print(\"F1 score for training set: {:.4f}.\"\n",
    "          .format(predict_labels(clf, X_train, y_train)))\n",
    "    print(\"F1 score for test set: {:.4f}.\"\n",
    "          .format(predict_labels(clf, X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Model Performance \n",
    "\n",
    "Let's test 3 supervised learning models:\n",
    "- A) Decision Trees\n",
    "- B) Support Vector Machines (SVM)\n",
    "- C) Gaussian Naive Bayes (GaussianNB)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_random_seed = 42\n",
    "\n",
    "# Initialize the three models\n",
    "clf_A = DecisionTreeClassifier(random_state=my_random_seed)\n",
    "clf_B = SVC(random_state=my_random_seed)\n",
    "clf_C = GaussianNB()\n",
    "\n",
    "\n",
    "# loop thru models, then thru train sizes\n",
    "for clf in [clf_A, clf_B, clf_C]:\n",
    "    print(\"\\n{}: \\n\".format(clf.__class__.__name__))\n",
    "    train_predict(clf, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "for clf in [clf_A, clf_B, clf_C]:\n",
    "    print('\\nReport for {}:\\n'.format(clf.__class__.__name__))\n",
    "    print(classification_report(y_test, clf.predict(X_test)))\n",
    "    print('-'*52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **precision** is the ratio ```tp / (tp + fp)``` where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "\n",
    "The **recall** is the ratio ```tp / (tp + fn)``` where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "The **F1 score** can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
    "\n",
    "```F1 = 2 * (precision * recall) / (precision + recall)```\n",
    "\n",
    "The **support** is the number of occurrences of each class in ```y_true```.\n",
    "\n",
    "Source:\n",
    "[sklearn.metrics.precision_recall_fscore_support](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html) [sklearn.metrics.f1_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Brief interpretation:**\n",
    "\n",
    "DecisionTreeClassifier has the best peformance and provides the best forecasts.\n",
    "\n",
    "Let's save and load the model just to double check that it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model.joblib'\n",
    "dump(clf_A, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log F1 score for experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_clf_A = f1_score(y_test, clf_A.predict(X_test))\n",
    "metrics = {\n",
    "    'metrics': [\n",
    "        {\n",
    "            'name': 'F1-score',\n",
    "            'numberValue': f1_score_clf_A,\n",
    "            'format': 'PERCENTAGE'\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('mlpipeline-metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
