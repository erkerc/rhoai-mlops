kind: Template
apiVersion: template.openshift.io/v1
metadata:
  name: triton
  namespace: redhat-ods-applications
  labels:
    opendatahub.io/dashboard: 'true'
  annotations:
    opendatahub.io/template-enabled: 'true'
    tags: 'triton,servingruntime'
objects:
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    metadata:
      name: triton
      labels:
        name: modelmesh-serving-triton-2.x-SR
      annotations:
        maxLoadingConcurrency: '2'
    spec:
      supportedModelFormats:
        - name: keras
          version: '2'
          autoSelect: true
        - name: onnx
          version: '1'
          autoSelect: true
        - name: pytorch
          version: '1'
          autoSelect: true
        - name: tensorflow
          version: '1'
          autoSelect: true
        - name: tensorflow
          version: '2'
          autoSelect: true
        - name: tensorrt
          version: '7'
          autoSelect: true
      protocolVersions:
        - grpc-v2
      multiModel: true
      grpcEndpoint: 'port:8085'
      grpcDataEndpoint: 'port:8001'
      containers:
        - name: triton
          image: 'nvcr.io/nvidia/tritonserver:21.06.1-py3'
          command:
            - /bin/sh
          args:
            - '-c'
            - >-
              mkdir -p /models/_triton_models; chmod 777 /models/_triton_models;
              exec tritonserver "--model-repository=/models/_triton_models"
              "--model-control-mode=explicit" "--strict-model-config=false"
              "--strict-readiness=false" "--allow-http=true"
              "--allow-sagemaker=false" 
          resources:
            requests:
              cpu: 500m
              memory: 1Gi
            limits:
              cpu: '5'
              memory: 1Gi
          livenessProbe:
            exec:
              command:
                - curl
                - '--fail'
                - '--silent'
                - '--show-error'
                - '--max-time'
                - '9'
                - 'http://localhost:8000/v2/health/live'
            initialDelaySeconds: 5
            periodSeconds: 30
            timeoutSeconds: 10
      builtInAdapter:
        serverType: triton
        runtimeManagementPort: 8001
        memBufferBytes: 134217728
        modelLoadingTimeoutMillis: 90000
---
kind: Template
apiVersion: template.openshift.io/v1
metadata:
  name: mlserver
  namespace: redhat-ods-applications
  labels:
    opendatahub.io/dashboard: 'true'
  annotations:
    opendatahub.io/template-enabled: 'true'
    tags: 'mlserver,servingruntime'
objects:
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    metadata:
      name: mlserver
      labels:
        name: modelmesh-serving-mlserver-1.x-SR
    spec:
      supportedModelFormats:
        - name: sklearn
          version: "0" # v0.23.1
          autoSelect: true
        - name: xgboost
          version: "1" # v1.1.1
          autoSelect: true
        - name: lightgbm
          version: "3" # v3.2.1
          autoSelect: true

      protocolVersions:
        - grpc-v2
      multiModel: true

      grpcEndpoint: "port:8085"
      grpcDataEndpoint: "port:8001"

      containers:
        - name: mlserver
          image: seldonio/mlserver:0.5.2
          env:
            - name: MLSERVER_MODELS_DIR
              value: "/models/_mlserver_models/"
            - name: MLSERVER_GRPC_PORT
              value: "8001"
            # default value for HTTP port is 8080 which conflicts with MMesh's
            # Litelinks port
            - name: MLSERVER_HTTP_PORT
              value: "8002"
            - name: MLSERVER_LOAD_MODELS_AT_STARTUP
              value: "false"
            # Set a dummy model name via environment so that MLServer doesn't
            # error on a RepositoryIndex call when no models exist
            - name: MLSERVER_MODEL_NAME
              value: dummy-model-fixme
            # Set server addr to localhost to ensure MLServer only listen inside the pod
            - name: MLSERVER_HOST
              value: "127.0.0.1"
            # Increase gRPC max message size to support larger payloads
            # Unlimited because it will be restricted at the model mesh layer
            - name: MLSERVER_GRPC_MAX_MESSAGE_LENGTH
              value: "-1"
          resources:
            requests:
              cpu: 500m
              memory: 1Gi
            limits:
              cpu: "5"
              memory: 1Gi
      builtInAdapter:
        serverType: mlserver
        runtimeManagementPort: 8001
        memBufferBytes: 134217728
        modelLoadingTimeoutMillis: 90000
---
kind: Template
apiVersion: template.openshift.io/v1
metadata:
  name: torchserve
  namespace: redhat-ods-applications
  labels:
    opendatahub.io/dashboard: 'true'
  annotations:
    opendatahub.io/template-enabled: 'true'
    tags: 'torchserve,servingruntime'
objects:
  - apiVersion: serving.kserve.io/v1alpha1
    kind: ServingRuntime
    metadata:
      name: torchserve
      labels:
        name: modelmesh-serving-torchserve-0.x-SR
    spec:
      supportedModelFormats:
        - name: pytorch-mar
          version: "0"
          autoSelect: true

      multiModel: true

      grpcEndpoint: "port:8085"
      grpcDataEndpoint: "port:7070"

      containers:
        - name: torchserve
          image: pytorch/torchserve:0.6.0-cpu
          args:
            # Adapter creates the config file; wait for it to exist before starting
            - while [ ! -e "$TS_CONFIG_FILE" ]; do echo "waiting for config file..."; sleep 1; done;
            - exec
            - torchserve
            - --start
            - --foreground
          env:
            - name: TS_CONFIG_FILE
              value: /models/_torchserve_models/mmconfig.properties
            # TBD, this may give better performance
            #- name: TS_PREFER_DIRECT_BUFFER
            #  value: true
            # Additional TS_ prefixed TorchServe config options may be added here
          resources:
            requests:
              cpu: 500m
              memory: 1Gi
            limits:
              cpu: "5"
              memory: 1Gi
      builtInAdapter:
        serverType: torchserve
        runtimeManagementPort: 7071
        memBufferBytes: 134217728
        modelLoadingTimeoutMillis: 90000